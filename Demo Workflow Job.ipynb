{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31da066a-0ac4-4216-8f4b-1b8c02338609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: /Users/your_name/random_user_etl\n",
    "# Language: Python\n",
    "\n",
    "# --- Setup & Imports ---\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "# Although not strictly needed for one user, we import Spark to highlight \n",
    "# the platform's scaling capability for big data tasks.\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# --- 1. Fetch Data (The 'Extraction' step) ---\n",
    "print(\"### STEP 1: Fetch Data (E) ###\")\n",
    "api_url = \"https://raw.githubusercontent.com/Canbirlik/public_demo/refs/heads/main/result.json\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "try:\n",
    "    response = requests.get(\n",
    "        api_url,\n",
    "        headers=headers,\n",
    "        timeout=10\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    raw_data = response.json()\n",
    "    user = raw_data['results'][0]\n",
    "    print(\"✅ Raw data successfully fetched from API.\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"HTTP error: {e}\")\n",
    "    print(\"Check if your Databricks workspace allows outbound internet access or if the API is blocking your IP.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    raise Exception(f\"API data fetching error: {e}\")\n",
    "\n",
    "# --- 2. Transform Data (The 'Transformation' step) ---\n",
    "print(\"\\n### STEP 2: Transform Data (T) ###\")\n",
    "processed_user = {\n",
    "    \"full_name\": f\"{user['name']['first']} {user['name']['last']}\",\n",
    "    \"email_address\": user['email'],\n",
    "    \"country\": user['location']['country'],\n",
    "    \"job_run_timestamp\": str(datetime.now())\n",
    "}\n",
    "print(\"⏳ User data successfully extracted and transformed.\")\n",
    "\n",
    "# (If this were Big Data, this is where we'd use Spark for massive scaling)\n",
    "# spark = SparkSession.builder.appName(\"APITransform\").getOrCreate()\n",
    "# df = spark.createDataFrame([processed_user])\n",
    "\n",
    "\n",
    "# --- 3. Load Results (The 'Loading' step) ---\n",
    "print(\"\\n### STEP 3: Load Results (L) ###\")\n",
    "# The output is logged directly in the Databricks Job run UI.\n",
    "print(\"--- LOADED CLEAN USER DATA ---\")\n",
    "print(f\"  - Full Name: {processed_user['full_name']}\")\n",
    "print(f\"  - Country: {processed_user['country']}\")\n",
    "\n",
    "# Actual loading logic (e.g., writing to a Delta Table)\n",
    "# Example: df.write.mode(\"append\").saveAsTable(\"default.processed_users\") \n",
    "\n",
    "print(\"✅ Job complete. Data loaded to Delta Lake (assumed).\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo Workflow Job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}